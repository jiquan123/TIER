# TIER

This is the official repo of the paper [TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment](https://arxiv.org/abs/2401.03854):
  ```
@misc{yuan2024tier,
      title={TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment}, 
      author={Jiquan Yuan and Xinyan Cao and Jinming Che and Qinyuan Wang and Sen Liang and Wei Ren and Jinlong Lin and Xixin Cao},
      year={2024},
      eprint={2401.03854},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
<hr />

> **Abstract:** *Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the quality of AI-generated images from a human perception perspective, has emerged as a new topic in computer vision. Unlike common image quality assessment tasks where images are derived from original ones distorted by noise, blur, and compression, in AIGCIQA tasks, images are typically generated by generative models using text prompts. Considerable efforts have been made in the past years to advance AIGCIQA. However, most existing AIGCIQA methods regress predicted scores directly from individual generated images, overlooking the information contained in the text prompts of these images. This oversight partially limits the performance of these AIGCIQA methods. To address this issue, we propose a text-image encoder-based regression (TIER) framework. Specifically, we process the generated images and their corresponding text prompts as inputs, utilizing a text encoder and an image encoder to extract features from these text prompts and generated images, respectively. To demonstrate the effectiveness of our proposed TIER method, we conduct extensive experiments on several mainstream AIGCIQA databases, including AGIQA-1K, AGIQA-3K, and AIGCIQA2023. The experimental results indicate that our proposed TIER method generally demonstrates superior performance compared to baseline in most cases.* 
<hr />


### Dataset
we conduct extensive experiments on three mainstream AIGCIQA databases, including: 
-  AGIQA-1K download [here](https://github.com/lcysyzxdxc/AGIQA-1k-Database)
-  AGIQA-3K download [here](https://github.com/lcysyzxdxc/AGIQA-3k-Database)
-  AIGCIQA2023 download [here](https://github.com/wangjiarui153/AIGCIQA2023)

### TIER
The pipeline of our proposed TIER framework.
![TIER](https://github.com/jiquan123/TIER/blob/main/Fig/TIER.png)

### Text encoder
In our paper, we choose BERT as the text encoder for extracting features from text prompts. They are:
-  bert-base-uncased
-  bert-large-uncased

It can be downloaded using this link: [[百度网盘](https://pan.baidu.com/s/19TDTIm6_0QJtm8N1YlfMjg) (提取码：text)].

or it can be accessed on [huggingface](https://huggingface.co/docs/transformers/installation).


### Image encoder
For feature extraction from input images, we selected several backbone
network models pre-trained on the ImageNet dataset as image encoder, including:
-  ResNet18 [weights](https://download.pytorch.org/models/resnet18-f37072fd.pth)
-  ResNet50 [weights](https://download.pytorch.org/models/resnet50-0676ba61.pth)
-  InceptionV4 [weights](http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth)


The log of our experiments is [here](https://github.com/jiquan123/TIER/blob/main/exp/TIER.log)

### Contact
If you have any question, please contact yuanjiquan@stu.pku.edu.cn
